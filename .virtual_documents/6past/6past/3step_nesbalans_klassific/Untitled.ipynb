





import pandas as pd
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
import numpy as np
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_curve 
from sklearn.metrics import roc_auc_score


data = pd.read_csv("")


data.head()


target = data['Claim']
features = data.drop('Claim', axis=1)
features_train, features_valid, target_train, target_valid = train_test_split(
    features, target, test_size=0.25, random_state=12345)
model = LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced')
model.fit(features_train, target_train)
predicted_valid = model.predict(features_valid)
print(f1_score(target_valid, predicted_valid))


target_zeros = target_train[target_train==0]
target_ones = target_train[target_train==1]


features_ones = features_train[target_train==1]
features_zeros = features_train[target_train==0]


target_zeros.shape


features_ones.shape


features_zeros.shape


target_ones.shape


repeat = 10 
features_upsampled = pd.concat([features_zeros]+[features_ones] * repeat)
target_upsampled = pd.concat([target_zeros]+[target_ones]*repeat)


features_upsampled.shape


target_upsampled.shape


shuffle(features_upsampled)



shuffle(target_upsampled)





def upsample(features,target,repeat):
    target_zeros = target[target==0]
    target_ones = target[target==1]
    features_ones = features[target==1]
    features_zeros = features[target==0]
    features_upsampled = pd.concat([features_zeros]+[features_ones] * repeat)
    target_upsampled = pd.concat([target_zeros]+[target_ones]*repeat)
    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)
    return features_upsampled, target_upsampled


features_upsampled, target_upsampled = upsample(features_train, target_train, 10)


features_upsampled


target_upsampled


model = LogisticRegression(solver="liblinear")
model.fit(features_upsampled, target_upsampled)
prediction1 = model.predict(features_valid)
f1 = f1_score(target_valid, prediction1)
print(f1)





def downsample(features,target,fraction):
    target_zeros = target[target==0]
    target_ones = target[target==1]
    features_ones = features[target==1]
    features_zeros = features[target==0]
    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state = 12345)]+[features_ones])
    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state = 12345)]+[target_ones])
    features_downsampled, target_downsampled = shuffle(features_downsampled,target_downsampled, random_state=12345)
    return features_downsampled, target_downsampled


features_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)
print(features_downsampled.shape)
print(target_downsampled.shape)


model = LogisticRegression(solver="liblinear")
model.fit(features_downsampled, target_downsampled)
prediction2 = model.predict(features_valid)
f1 = f1_score(target_valid, prediction2)
print(f1)





model = LogisticRegression(random_state=12345, solver='liblinear')
model.fit(features_train, target_train)
probabilities_valid = model.predict_proba(features_valid)  #вероятность классов (1 столбец - вероятность отрицательного класса, 2 столбец - положительного)
probabilities_one_valid = probabilities_valid[:,1]
print(probabilities_one_valid)


probabilities_valid


target = data['Claim']
features = data.drop('Claim', axis=1)
features_train, features_valid, target_train, target_valid = train_test_split(
features, target, test_size=0.25, random_state=12345)
model = LogisticRegression(random_state=12345, solver='liblinear')
model.fit(features_train, target_train)
probabilities_valid = model.predict_proba(features_valid)
probabilities_one_valid = probabilities_valid[:, 1]

for i in np.arange(0, 0.3, 0.02):
    predicted_valid = probabilities_one_valid>i
    precision = precision_score(target_valid,predicted_valid)
    recall = recall_score(target_valid,predicted_valid)
    if (precision+recall) !=0:
        f1 = 2 * recall * precision /(precision+recall)
        print("Порог = {:.2f} | Точность = {:.3f}, Полнота = {:.3f},  f1 = {:.3f}".format(i, precision, recall, f1))
    else:
        break














model = LogisticRegression(random_state=12345, solver='liblinear')
model.fit(features_train, target_train)

probabilities_valid = model.predict_proba(features_valid)
precision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])

plt.figure(figsize=(6, 6))
plt.step(recall, precision, where='post')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Кривая Precision-Recall')
plt.show() 


fpr,tpr,thresholds = roc_curve(target_valid, probabilities_valid[:, 1])
plt.figure()
plt.plot([0, 1], [0, 1], linestyle='--')
plt.plot(fpr,tpr)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel("False Positive Rate")
plt.ylabel('True Positive Rate')
plt.title('ROC-кривая')
plt.show()


roc_auc = roc_auc_score(target_valid, probabilities_valid[:, 1])
print(roc_auc)


jupyter labextension install @lckr/jupyterlab_variablehinter


pip install jupyterlab-lsp python-lsp-server[all]


pip uninstall jupyterlab-lsp python-lsp-server
jupyter lab clean
jupyter lab build


pip install --upgrade ipython jedi


pd.to_Series
